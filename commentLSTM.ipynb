{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, import the necessary modules in order to create the LSTM and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import tensorflow\n",
    "import keras as keras\n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "tensorflow.random.set_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a an array which contains all of the comments from our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NagoyaComments = pd.read_csv('NagoyaComments.csv')\n",
    "Comments = []\n",
    "\n",
    "for x in NagoyaComments.Comments:\n",
    "    Comments.append(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to clean the data in our CSV so it can be used to to train the model, so we remove punctuation and convert it all to  lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i made this ',\n",
       " 'whoever is reading this i hope you will get everything you dream of in 2021',\n",
       " 'listening to chill lofi beats really helps in studying\\n albert einstein probably',\n",
       " 'tbh you could put anything uncle iroh says put it over a lofi beat and the songs quality would go up by 10000',\n",
       " 'the easy south america naively ski because pantry reversely worry athwart a careful brake fearful fearless superficial passbook',\n",
       " 'this might be a cringy heart opening comment but this year i was supposed to go to nagoya for a couple of months with an scholarship since i was a teenager it was my dream to live in japan for a while and i worked super hard for it but the pandemic came and my country and pretty much the whole world was closed a week before i was going to take the planei have listened to japan inspired mixes for a couple of years now longing for the moment of going there now they hit in a different wayedit since this commment is getting a lot of attention i think its fair to give an updateso the scholarship kept getting pushed for a later date first was from march 2020 to august 2020 then august 2020 to march 2021 and everything seemed to be going well till the this latest wave in which that the japanese government have decided to close borders once again till february for nowas such the university decided to push the scholarship date to august 2021on a personal note i had been expecting this would happen so it doesnt hurt as much as the first time did although you can imagine it doesnt feel any better i am starting to feel like i am losing time waiting for this when i could be looking for an stable job at this point or something alike so i will try to get there this last time and leave it at that if it cant happen sometimes dreams dont come true and thats ok it doesnt mean that the hardships you went through for them were for nothing and i have lately been finding a new appreciation for a lot of things that i was blinded from by my desire to push through also it really warms my heart that this comment has become somewhat of an internet checkpoint and i wanted to say thank you i hope you all find what you are looking for whatever it is',\n",
       " 'sometimes lifes a burden but dont focus on that instead try to analize focus on the light make thing less heavier with your mind cuz afterall maybe thats why we are here for like fallen angels trying to reach god',\n",
       " 'for those of you who have managed to see this comment im wishing you the best in life and i hope you are doing well ',\n",
       " 'the bouncy goodbye simplistically bolt because dew immunophenotypically punish midst a caring santa polite zealous security',\n",
       " 'not me crying as soon as i heard uncle iroh tt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in Comments]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenise the entries in our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 574],\n",
       " [7, 574, 12],\n",
       " [541, 13],\n",
       " [541, 13, 234],\n",
       " [541, 13, 234, 12],\n",
       " [541, 13, 234, 12, 7],\n",
       " [541, 13, 234, 12, 7, 106],\n",
       " [541, 13, 234, 12, 7, 106, 3],\n",
       " [541, 13, 234, 12, 7, 106, 3, 35],\n",
       " [541, 13, 234, 12, 7, 106, 3, 35, 60]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad out the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow I Just Ordered Pokemon Cards From\n",
      "I Don'T Was Listening To This While Struggling With My Homework When All Of A Sudden I Heard\n",
      "Heres Fact You Are Someday I Heard Uncle Iroh Tt Going To Cry That It Way That This Hits So Chill\n",
      "Really Does Anybody Love The Comments\n",
      "Sometimes You For This Is Now I Have A Million Qestions Rn Here\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"wow\", 6, model, max_sequence_len))\n",
    "print (generate_text(\"I don't\", 16, model, max_sequence_len))\n",
    "print (generate_text(\"Heres\", 20, model, max_sequence_len))\n",
    "print (generate_text(\"Really\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"Sometimes\", 12, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
